Full-text links: Download: PDF Other formats (license) Current browse context: cs.CR < prev | next > new | recent | 1609Change to browse by: cscs.LGstatstat.ML References & CitationsNASA ADS DBLP - CS Bibliography listing | bibtex Florian TramèrFan ZhangAri JuelsMichael K. ReiterThomas Ristenpart Bookmark (what is this?) Computer Science > Cryptography and Security Title: Stealing Machine Learning Models via Prediction APIs Authors: Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, Thomas Ristenpart (Submitted on 9 Sep 2016 (v1), last revised 3 Oct 2016 (this version, v2)) Abstract: Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service ("predictive analytics") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., "steal") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures. Comments: 19 pages, 7 figures, Proceedings of USENIX Security 2016 Subjects: Cryptography and Security (cs.CR); Learning (cs.LG); Machine Learning (stat.ML) Cite as: arXiv:1609.02943 [cs.CR] (or arXiv:1609.02943v2 [cs.CR] for this version) Submission history From: Florian Tramèr [view email] [v1] Fri, 9 Sep 2016 20:39:20 GMT (1413kb,D)[v2] Mon, 3 Oct 2016 02:44:14 GMT (1413kb,D) Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)mathjaxToggle();
Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service ("predictive analytics") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., "steal") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures.