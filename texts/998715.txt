The Mythos of Model Interpretability Zachary C. Lipton 1 Abstract no one has managed to set it in writing, or (ii) the term in- terpretability is ill-defined, and thus claims regarding inter- Supervised machine learning models boast re- pretability of various models may exhibit a quasi-scientific markable predictive capabilities. But can you arXiv:1606.03490v3 [cs.LG] 6 Mar 2017 character. Our investigation of the literature suggests the trust your model? Will it work in deployment? latter to be the case. Both the motives for interpretability What else can it tell you about the world? We and the technical descriptions of interpretable models are want models to be not only good, but inter- diverse and occasionally discordant, suggesting that inter- pretable. And yet the task of interpretation ap- pretability refers to more than one concept. In this paper, pears underspecified. Papers provide diverse and we seek to clarify both, suggesting that interpretability is sometimes non-overlapping motivations for in- not a monolithic concept, but in fact reflects several dis- terpretability, and offer myriad notions of what tinct ideas. We hope, through this critical analysis, to bring attributes render models interpretable. Despite focus to the dialogue. this ambiguity, many papers proclaim inter- pretability axiomatically, absent further explana- Here, we mainly consider supervised learning and not other tion. In this paper, we seek to refine the dis- machine learning paradigms, such as reinforcement learn- course on interpretability. First, we examine the ing and interactive learning. This scope derives from our motivations underlying interest in interpretabil- original interest in the oft-made claim that linear models ity, finding them to be diverse and occasionally are preferable to deep neural networks on account of their discordant. Then, we address model properties interpretability (Lou et al., 2012). To gain conceptual clar- and techniques thought to confer interpretability, ity, we ask the refining questions: What is interpretability identifying transparency to humans and post-hoc and why is it important? Broadening the scope of discus- explanations as competing notions. Throughout, sion seems counterproductive with respect to our aims. For we discuss the feasibility and desirability of dif- research investigating interpretability in the context of rein- ferent notions, and question the oft-made asser- forcement learning, we point to (Dragan et al., 2013) which tions that linear models are interpretable and that studies the human interpretability of robot actions. By the deep neural networks are not. same reasoning, we do not delve as much as other papers might into Bayesian methods, however try to draw these connections where appropriate. 1. Introduction To ground any discussion of what might constitute inter- pretability, we first consider the various desiderata put forth As machine learning models penetrate critical areas like in work addressing the topic (expanded in §2). Many pa- medicine, the criminal justice system, and financial mar- pers propose interpretability as a means to engender trust kets, the inability of humans to understand these mod- (Kim, 2015; Ridgeway et al., 1998). But what is trust? els seems problematic (Caruana et al., 2015; Kim, 2015). Does it refer to faith in a model’s performance (Ribeiro Some suggest model interpretability as a remedy, but few et al., 2016), robustness, or to some other property of the articulate precisely what interpretability means or why it is decisions it makes? Does interpretability simply mean a important. Despite the absence of a definition, papers fre- low-level mechanistic understanding of our models? If quently make claims about the interpretability of various so does it apply to the features, parameters, models, or models. From this, we might conclude that either: (i) the training algorithms? Other papers suggest a connection definition of interpretability is universally agreed upon, but between an interpretable model and one which uncovers 1 University of California, San Diego. Correspondence to: causal structure in data (Athey & Imbens, 2015). The legal Zachary C. Lipton <zlipton@cs.ucsd.edu>. notion of a right to explanation offers yet another lens on interpretability. 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY, USA. Copyright by the Often, our machine learning problem formulations are im- author(s).
The Mythos of Model Interpretability perfect matches for the real-life tasks they are meant to 2. Desiderata of Interpretability Research solve. This can happen when simplified optimization ob- jectives fail to capture our more complex real-life goals. At present, interpretability has no formal technical mean- Consider medical research with longitudinal data. Our real ing. One aim of this paper is to propose more specific def- goal may be to discover potentially causal associations, as initions. Before we can determine which meanings might with smoking and cancer (Wang et al., 1999). But the opti- be appropriate, we must ask what the real-world objectives mization objective for most supervised learning models is of interpretability research are. In this section we spell out simply to minimize error, a feat that might be achieved in a the various desiderata of interpretability research through purely correlative fashion. the lens of the literature. Another such divergence of real-life and machine learning While these desiderata are diverse, it might be instructive problem formulations emerges when the off-line training to first consider a common thread that persists throughout data for a supervised learner is not perfectly representative the literature: The demand for interpretability arises when of the likely deployment environment. For example, the there is a mismatch between the formal objectives of super- environment is typically not stationary. This is the case for vised learning (test set predictive performance) and the real product recommendation, as new products are introduced world costs in a deployment setting. and preferences for some items shift daily. In more ex- treme cases, actions influenced by a model may alter the environment, invalidating future predictions. Discussions of interpretability sometimes suggest that hu- man decision-makers are themselves interpretable because Evaluation they can explain their actions (Ridgeway et al., 1998). But Metric precisely what notion of interpretability do these expla- nations satisfy? They seem unlikely to clarify the mech- anisms or the precise algorithms by which brains work. Interpretation Nevertheless, the information conferred by an interpreta- tion may be useful. Thus, one purpose of interpretations may be to convey useful information of any kind. After addressing the desiderata of interpretability, we con- Figure 1. Typically, evaluation metrics require only predictions sider what properties of models might render them in- and ground truth labels. When stakeholders additionally demand interpretability, we might infer the existence of desiderata that terpretable (expanded in §3). Some papers equate in- cannot be captured in this fashion. terpretability with understandability or intelligibility (Lou et al., 2013), i.e., that we can grasp how the models work. In these papers, understandable models are some- times called transparent, while incomprehensible models Consider that most common evaluation metrics for su- are called black boxes. But what constitutes transparency? pervised learning require only predictions, together with We might look to the algorithm itself. Will it converge? ground truth, to produce a score. These metrics can be Does it produce a unique solution? Or we might look to its be assessed for every supervised learning model. So the parameters: do we understand what each represents? Al- very desire for an interpretation suggests that in some sce- ternatively, we could consider the model’s complexity. Is it narios, predictions alone and metrics calculated on these simple enough to be examined all at once by a human? predictions do not suffice to characterize the model (Figure Other papers investigate so-called post-hoc interpretations. 1). We should then ask, what are these other desiderata and These interpretations might explain predictions without under what circumstances are they sought? elucidating the mechanisms by which models work. Ex- However inconveniently, it turns out that many situations amples of post-hoc interpretations include the verbal ex- arise when our real world objectives are difficult to en- planations produced by people or the saliency maps used code as simple real-valued functions. For example, an al- to analyze deep neural networks. Thus, humans decisions gorithm for making hiring decisions should simultaneously might admit post-hoc interpretability despite the black box optimize productivity, ethics, and legality. But typically, nature of human brains, revealing a contradiction between ethics and legality cannot be directly optimized. The prob- two popular notions of interpretability. lem can also arise when the dynamics of the deployment environment differ from the training environment. In all cases, interpretations serve those objectives that we deem important but struggle to model formally.
The Mythos of Model Interpretability 2.1. Trust (Pearl, 2009). But these methods tend to rely on strong assumptions of prior knowledge. Some papers motivate interpretability by suggesting it to be prerequisite for trust (Kim, 2015; Ribeiro et al., 2016). But 2.3. Transferability what is trust? Is it simply confidence that a model will per- form well? If so, a sufficiently accurate model should be Typically we choose training and test data by randomly par- demonstrably trustworthy and interpretability would serve titioning examples from the same distribution. We then no purpose. Trust might also be defined subjectively. For judge a model’s generalization error by the gap between example, a person might feel more at ease with a well- its performance on training and test data. However, hu- understood model, even if this understanding served no ob- mans exhibit a far richer capacity to generalize, transfer- vious purpose. Alternatively, when the training and deploy- ring learned skills to unfamiliar situations. We already use ment objectives diverge, trust might denote confidence that machine learning algorithms in situations where such abil- the model will perform well with respect to the real objec- ities are required, such as when the environment is non- tives and scenarios. stationary. We also deploy models in settings where their For example, consider the growing use of machine learning use might alter the environment, invalidating their future models to forecast crime rates for purposes of allocating predictions. Along these lines, Caruana et al. (2015) de- police officers. We may trust the model to make accurate scribe a model trained to predict probability of death from predictions but not to account for racial biases in the train- pneumonia that assigned less risk to patients if they also ing data for the model’s own effect in perpetuating a cy- had asthma. In fact, asthma was predictive of lower risk of cle of incarceration by over-policing some neighborhoods. death. This owed to the more aggressive treatment these Another sense in which we might trust a machine learn- patients received. But if the model were deployed to aid ing model might be that we feel comfortable relinquishing in triage, these patients would then receive less aggressive control to it. In this sense, we might care not only about treatment, invalidating the model. how often a model is right but also for which examples it Even worse, we could imagine situations, like machine is right. If the model tends to make mistakes in regions learning for security, where the environment might be ac- of input space where humans also make mistakes, and is tively adversarial. Consider the recently discovered sus- typically accurate when humans are accurate, then it may ceptibility of convolutional neural networks (CNNs) to ad- be considered trustworthy in the sense that there is no ex- versarial examples. The CNNs were made to misclassify pected cost of relinquishing control. But if a model tends to images that were imperceptibly (to a human) perturbed make mistakes for inputs that humans classify accurately, (Szegedy et al., 2013). Of course, this isn’t overfitting in then there may always be an advantage to maintaining hu- the classical sense. The results achieved on training data man supervision of the algorithms. generalize well to i.i.d. test data. But these are mistakes a human wouldn’t make and we would prefer models not to 2.2. Causality make these mistakes either. Although supervised learning models are only optimized Already, supervised learning models are regularly subject directly to make associations, researchers often use them to such adversarial manipulation. Consider the models in the hope of inferring properties or generating hypotheses used to generate credit ratings, scores that when higher about the natural world. For example, a simple regression should signify a higher probability that an individual re- model might reveal a strong association between thalido- pays a loan. According to their own technical report, FICO mide use and birth defects or smoking and lung cancer trains credit models using logistic regression (Fair Isaac (Wang et al., 1999). Corporation, 2011), specifically citing interpretability as a motivation for the choice of model. Features include The associations learned by supervised learning algorithms dummy variables representing binned values for average are not guaranteed to reflect causal relationships. There age of accounts, debt ratio, and the number of late pay- could always exist unobserved causes responsible for both ments, and the number of accounts in good standing. associated variables. One might hope, however, that by interpreting supervised learning models, we could gener- Several of these factors can be manipulated at will by ate hypotheses that scientists could then test experimen- credit-seekers. For example, one’s debt ratio can be im- tally. Liu et al. (2005), for example, emphasizes regression proved simply by requesting periodic increases to credit trees and Bayesian neural networks, suggesting that mod- lines while keeping spending patterns constant. Similarly, els are interpretable and thus better able to provide clues the total number of accounts can be increased by simply about the causal relationships between physiologic signals applying for new accounts, when the probability of accep- and affective states. The task of inferring causal relation- tance is reasonably high. Indeed, FICO and Experian both ships from observational data has been extensively studied acknowledge that credit ratings can be manipulated, even
The Mythos of Model Interpretability suggesting guides for improving one’s credit rating. These Recidivism predictions are already used to determine who rating improvement strategies do not fundamentally change to release and who to detain, raising ethical concerns. 1 one’s underlying ability to pay a debt. The fact that individ- How can we be sure that predictions do not discriminate on uals actively and successfully game the rating system may the basis of race? Conventional evaluation metrics such as invalidate its predictive power. accuracy or AUC offer little assurance that a model and via decision theory, its actions, behave acceptably. Thus de- 2.4. Informativeness mands for fairness often lead to demands for interpretable models. Sometimes we apply decision theory to the outputs of su- pervised models to take actions in the real world. However, New regulations in the European Union propose that indi- in another common use paradigm, the supervised model viduals affected by algorithmic decisions have a right to is used instead to provide information to human decision explanation (Goodman & Flaxman, 2016). Precisely what makers, a setting considered by Kim et al. (2015); Huys- form such an explanation might take or how such an ex- mans et al. (2011). While the machine learning objec- planation could be proven correct and not merely appeas- tive might be to reduce error, the real-world purpose is to ing remain open questions. Moreover the same regulations provide useful information. The most obvious way that a suggest that algorithmic decisions should be contestable. model conveys information is via its outputs. However, it So in order for such explanations to be useful it seems they may be possible via some procedure to convey additional must (i) present clear reasoning based on falsifiable propo- information to the human decision-maker. sitions and (ii) offer some natural way of contesting these propositions and modifying the decisions appropriately if By analogy, we might consider a PhD student seeking ad- they are falsified. vice from her advisor. Suppose the student asks what venue would best suit a paper. The advisor could simply name one conference, but this may not be especially useful. Even if 3. Properties of Interpretable Models the advisor is reasonably intelligent, the terse reply doesn’t We turn now to consider the techniques and model proper- enable the student to meaningfully combine the advisor’s ties that are proposed either to enable or to comprise inter- knowledge with her own. pretations. These broadly fall into two categories. The first An interpretation may prove informative even without relates to transparency, i.e., how does the model work? The shedding light on a model’s inner workings. For exam- second consists of post-hoc explanations, i.e., what else ple, a diagnosis model might provide intuition to a human can the model tell me? This division is a useful organi- decision-maker by pointing to similar cases in support of a zationally, but we note that it is not absolute. For example diagnostic decision. In some cases, we train a supervised post-hoc analysis techniques attempt to uncover the signif- learning model, but our real task more closely resembles icance of various of parameters, an aim we group under the unsupervised learning. Here, our real goal is to explore the heading of transparency. data and the objective serves only as weak supervision. 3.1. Transparency 2.5. Fair and Ethical Decision-Making Informally, transparency is the opposite of opacity or At present, politicians, journalists and researchers have blackbox-ness. It connotes some sense of understanding expressed concern that we must produce interpretations the mechanism by which the model works. We consider for the purpose of assessing whether decisions produced transparency at the level of the entire model (simulatabil- automatically by algorithms conform to ethical standards ity), at the level of individual components (e.g. parameters) (Goodman & Flaxman, 2016). (decomposability), and at the level of the training algorithm (algorithmic transparency). The concerns are timely: Algorithmic decision-making mediates more and more of our interactions, influencing 3.1.1. S IMULATABILITY our social experiences, the news we see, our finances, and our career opportunities. We task computer programs with In the strictest sense, we might call a model transparent if approving lines of credit, curating news, and filtering job a person can contemplate the entire model at once. This applicants. Courts even deploy computerized algorithms to definition suggests that an interpretable model is a simple predict risk of recidivism, the probability that an individual model. We might think, for example that for a model to relapses into criminal behavior (Chouldechova, 2016). It be fully understood, a human should be able to take the seems likely that this trend will only accelerate as break- 1 It seems reasonable to argue that under most circumstances, throughs in artificial intelligence rapidly broaden the capa- risk-based punishment is fundamentally unethical, but this discus- bilities of software. sion requires exceeds the present scope.
The Mythos of Model Interpretability input data together with the parameters of the model and in tion, even for previously unseen datasets. This may give reasonable time step through every calculation required to some confidence that the model might behave in an online produce a prediction. This accords with the common claim setting requiring programmatic retraining on previously that sparse linear models, as produced by lasso regression unseen data. On the other hand, modern deep learning (Tibshirani, 1996), are more interpretable than dense linear methods lack this sort of algorithmic transparency. While models learned on the same inputs. Ribeiro et al. (2016) the heuristic optimization procedures for neural networks also adopt this notion of interpretability, suggesting that an are demonstrably powerful, we don’t understand how they interpretable model is one that “can be readily presented to work, and at present cannot guarantee a priori that they will the user with visual or textual artifacts.” work on new problems. Note, however, that humans exhibit none of these forms of transparency. For some models, such as decision trees, the size of the model (total number of nodes) may grow much faster than the time to perform inference (length of pass from root to 3.2. Post-hoc Interpretability leaf). This suggests that simulatability may admit two sub- Post-hoc interpretability presents a distinct approach to ex- types, one based on the total size of the model and another tracting information from learned models. While post- based on the computation required to perform inference. hoc interpretations often do not elucidate precisely how a Fixing a notion of simulatability, the quantity denoted by model works, they may nonetheless confer useful infor- reasonable is subjective. But clearly, given the limited mation for practitioners and end users of machine learn- capacity of human cognition, this ambiguity might only ing. Some common approaches to post-hoc interpreta- span several orders of magnitude. In this light, we suggest tions include natural language explanations, visualizations that neither linear models, rule-based systems, nor deci- of learned representations or models, and explanations by sion trees are intrinsically interpretable. Sufficiently high- example (e.g. this tumor is classified as malignant because dimensional models, unwieldy rule lists, and deep decision to the model it looks a lot like these other tumors). trees could all be considered less transparent than compar- To the extent that we might consider humans to be inter- atively compact neural networks. pretable, it is this sort of interpretability that applies. For all we know, the processes by which we humans make de- 3.1.2. D ECOMPOSABILITY cisions and those by which we explain them may be dis- A second notion of transparency might be that each part of tinct. One advantage of this concept of interpretability is the model - each input, parameter, and calculation - admits that we can interpret opaque models after-the-fact, without an intuitive explanation. This accords with the property of sacrificing predictive performance. intelligibility as described by (Lou et al., 2012). For exam- ple, each node in a decision tree might correspond to a plain 3.2.1. T EXT E XPLANATIONS text description (e.g. all patients with diastolic blood pres- Humans often justify decisions verbally. Similarly, we sure over 150). Similarly, the parameters of a linear model might train one model to generate predictions and a sep- could be described as representing strengths of association arate model, such as a recurrent neural network language between each feature and the label. model, to generate an explanation. Such an approach is Note that this notion of interpretability requires that in- taken in a line of work by Krening et al. (2016). They pro- puts themselves be individually interpretable, disqualifying pose a system in which one model (a reinforcement learner) some models with highly engineered or anonymous fea- chooses actions to optimize cumulative discounted return. tures. While this notion is popular, we shouldn’t accept it They train another model to map a model’s state represen- blindly. The weights of a linear model might seem intuitive, tation onto verbal explanations of strategy. These explana- but they can be fragile with respect to feature selection and tions are trained to maximize the likelihood of previously pre-processing. For example, associations between flu risk observed ground truth explanations from human players, and vaccination might be positive or negative depending and may not faithfully describe the agent’s decisions, how- on whether the feature set includes indicators of old age, ever plausible they appear. We note a connection between infancy, or immunodeficiency. this approach and recent work on neural image caption- ing in which the representations learned by a discriminative 3.1.3. A LGORITHMIC T RANSPARENCY convolutional neural network (trained for image classifica- tion) are co-opted by a second model to generate captions. A final notion of transparency might apply at the level of These captions might be regarded as interpretations that ac- the learning algorithm itself. For example, in the case of company classifications. linear models, we understand the shape of the error surface. We can prove that training will converge to a unique solu- In work on recommender systems, McAuley & Leskovec (2013) use text to explain the decisions of a latent factor
The Mythos of Model Interpretability model. Their method consists of simultaneously training a monyan et al., 2013; Wang et al., 2015). latent factor model for rating prediction and a topic model Note that these explanations of what a model is focusing for product reviews. During training they alternate between on may be misleading. The saliency map is a local expla- decreasing the squared error on rating prediction and in- nation only. Once you move a single pixel, you may get creasing the likelihood of review text. The models are con- a very different saliency map. This contrasts with linear nected because they use normalized latent factors as topic models, which model global relationships between inputs distributions. In other words, latent factors are regularized and outputs. such that they are also good at explaining the topic distri- butions in review text. The authors then explain user-item compatibility by examining the top words in the topics cor- responding to matching components of their latent factors. Note that the practice of interpreting topic models by pre- senting the top words is itself a post-hoc interpretation tech- nique that has invited scrutiny (Chang et al., 2009). 3.2.2. V ISUALIZATION Another common approach to generating post-hoc interpre- tations is to render visualizations in the hope of determin- ing qualitatively what a model has learned. One popular approach is to visualize high-dimensional distributed rep- resentations with t-SNE (Van der Maaten & Hinton, 2008), a technique that renders 2D visualizations in which nearby data points are likely to appear close together. Figure 2. Saliency map by Wang et al. (2015) to convey intuition Mordvintsev et al. (2015) attempt to explain what an im- over what the value function and advantage function portions of age classification network has learned by altering the input their deep Q-network are focusing on. through gradient descent to enhance the activations of cer- tain nodes selected from the hidden layers. An inspection Another attempt at local explanations is made by Ribeiro of the perturbed inputs can give clues to what the model et al. (2016). In this work, the authors explain the deci- has learned. Likely because the model was trained on a sions of any model in a local region near a particular point, large corpus of animal images, they observed that enhanc- by learning a separate sparse linear model to explain the ing some nodes caused the dog faces to appear throughout decisions of the first. the input image. In the computer vision community, similar approaches 3.2.4. E XPLANATION BY E XAMPLE have been explored to investigate what information is re- One post-hoc mechanism for explaining the decisions of a tained at various layers of a neural network. Mahendran model might be to report (in addition to predictions) which & Vedaldi (2015) pass an image through a discriminative other examples the model considers to be most similar, a convolutional neural network to generate a representation. method suggested by Caruana et al. (1999). After training They then demonstrate that the original image can be re- a deep neural network or latent variable model for a dis- covered with high fidelity even from reasonably high-level criminative task, we then have access not only to predic- representations (level 6 of an AlexNet) by performing gra- tions but also to the learned representations. Then, for any dient descent on randomly initialized pixels. example, in addition to generating a prediction, we can use the activations of the hidden layers to identify the k-nearest 3.2.3. L OCAL E XPLANATIONS neighbors based on the proximity in the space learned by While it may be difficult to succinctly describe the full the model. This sort of explanation by example has prece- mapping learned by a neural network, some papers focus dent in how humans sometimes justify actions by analogy. instead on explaining what a neural network depends on For example, doctors often refer to case studies to support locally. One popular approach for deep neural nets is to a planned treatment protocol. compute a saliency map. Typically, they take the gradient In the neural network literature, Mikolov et al. (2013) use of the output corresponding to the correct class with re- such an approach to examine the learned representations of spect to a given input vector. For images, this gradient can words after word2vec training. While their model is trained be applied as a mask (Figure 2), highlighting regions of the for discriminative skip-gram prediction, to examine what input that, if changed, would most influence the output (Si- relationships the model has learned, they enumerate near-
The Mythos of Model Interpretability est neighbors of words based on distances calculated in the 4.3. In some cases, transparency may be at odds with latent space. We also point to related work in Bayesian the broader objectives of AI methods: Kim et al. (2014) and Doshi-Velez et al. (2015) Some arguments against black-box algorithms appear to investigate cased-base reasoning approaches for interpret- preclude any model that could match or surpass our abil- ing generative models. ities on complex tasks. As a concrete example, the short- term goal of building trust with doctors by developing 4. Discussion transparent models might clash with the longer-term goal of improving health care. We should be careful when giv- The concept of interpretability appears simultaneously im- ing up predictive power, that the desire for transparency is portant and slippery. Earlier, we analyzed both the motiva- justified and isn’t simply a concession to institutional bi- tions for interpretability and some attempts by the research ases against new methods. community to confer it. In this discussion, we consider the implications of our analysis and offer several takeaways to the reader. 4.4. Post-hoc interpretations can potentially mislead We caution against blindly embracing post-hoc notions of 4.1. Linear models are not strictly more interpretable interpretability, especially when optimized to placate sub- than deep neural networks jective demands. In such cases, one might - deliberately or not - optimize an algorithm to present misleading but plau- Despite this claim’s enduring popularity, its truth content sible explanations. As humans, we are known to engage in varies depending on what notion of interpretability we em- this behavior, as evidenced in hiring practices and college ploy. With respect to algorithmic transparency, this claim admissions. Several journalists and social scientists have seems uncontroversial, but given high dimensional or heav- demonstrated that acceptance decisions attributed to virtues ily engineered features, linear models lose simulatability or like leadership or originality often disguise racial or gender decomposability, respectively. discrimination (Mounk, 2014). In the rush to gain accep- When choosing between linear and deep models, we must tance for machine learning and to emulate human intelli- often make a trade-off between algorithic transparency and gence, we should be careful not to reproduce pathological decomposability. This is because deep neural networks behavior at scale. tend to operate on raw or lightly processed features. So if nothing else, the features are intuitively meaningful, and 4.5. Future Work post-hoc reasoning is sensible. However, in order to get comparable performance, linear models often must operate We see several promising directions for future work. First, on heavily hand-engineered features. Lipton et al. (2016) for some problems, the discrepancy between real-life and demonstrates such a case where linear models can only ap- machine learning objectives could be mitigated by develop- proach the performance of RNNs at the cost of decompos- ing richer loss functions and performance metrics. Exem- ability. plars of this direction include research on sparsity-inducing regularizers and cost-sensitive learning. Second, we can For some kinds of post-hoc interpretation, deep neural net- expand this analysis to other ML paradigms such as rein- works exhibit a clear advantage. They learn rich represen- forcement learning. Reinforcement learners can address tations that can be visualized, verbalized, or used for clus- some (but not all) of the objectives of interpretability re- tering. Considering the desiderata for interpretability, lin- search by directly modeling interaction between models ear models appear to have a better track record for studying and environments. However, this capability may come at the natural world but we do not know of a theoretical reason the cost of allowing models to experiment in the world, in- why this must be so. Conceivably, post-hoc interpretations curring real consequences. Notably, reinforcement learners could prove useful in similar scenarios. are able to learn causal relationships between their actions and real world impacts. However, like supervised learn- 4.2. Claims about interpretability must be qualified ing, reinforcement learning relies on a well-defined scalar objective. For problems like fairness, where we struggle As demonstrated in this paper, the term does not reference to verbalize precise definitions of success, a shift of ML a monolithic concept. To be meaningful, any assertion paradigm is unlikely to eliminate the problems we face. regarding interpretability should fix a specific definition. If the model satisfies a form of transparency, this can be shown directly. For post-hoc interpretability, papers ought 5. Contributions to fix a clear objective and demonstrate evidence that the This paper identifies an important but under-recognized offered form of interpretation achieves it. problem: the term interpretability holds no agreed upon
The Mythos of Model Interpretability meaning, and yet machine learning conferences fre- Dragan, Anca D, Lee, Kenton CT, and Srinivasa, Sid- quently publish papers which wield the term in a quasi- dhartha S. Legibility and predictability of robot motion. mathematical way. For these papers to be meaningful and In Human-Robot Interaction (HRI), 2013 8th ACM/IEEE for this field to progress, we must critically engage the is- International Conference on. IEEE, 2013. sue of problem formulation. Moreover, we identify the in- compatibility of most presently investigated interpretability Fair Isaac Corporation. Introduction to scorecard for techniques with pressing problems facing machine learning fico model builder, 2011. URL http://www. in the wild. For example, little in the published work on fico.com/en/node/8140?file=7900. Ac- model intepretability addresses the idea of contestability. cessed: 2017-02-22. This paper makes a first step towards providing a compre- Goodman, Bryce and Flaxman, Seth. European union reg- hensive taxonomy of both the desiderata and methods in ulations on algorithmic decision-making and a” right to interpretability research. We argue that the paucity of criti- explanation”. arXiv:1606.08813, 2016. cal writing in the machine learning community is problem- atic. When we have solid problem formulations, flaws in Huysmans, Johan, Dejaeger, Karel, Mues, Christophe, methodology can be addressed by articulating new meth- Vanthienen, Jan, and Baesens, Bart. An empirical evalu- ods. But when the problem formulation itself is flawed, ation of the comprehensibility of decision table, tree and neither algorithms nor experiments are sufficient to address rule based predictive models. Decision Support Systems, the underlying problem. 2011. Moreover, as machine learning continues to exert influence Kim, Been. Interactive and interpretable machine learning upon society, we must be sure that we are solving the right models for human machine collaboration. PhD thesis, problems. While lawmakers and policymakers must in- Massachusetts Institute of Technology, 2015. creasingly consider the impact of machine learning, the re- sponsibility to account for the impact of machine learning Kim, Been, Rudin, Cynthia, and Shah, Julie A. The and to ensure its alignment with societal desiderata must Bayesian Case Model: A generative approach for case- ultimately be shared by practitioners and researchers in the based reasoning and prototype classification. In NIPS, field. Thus, we believe that such critical writing ought to 2014. have a voice at machine learning conferences. Kim, Been, Glassman, Elena, Johnson, Brittney, and Shah,
